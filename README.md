# Thrombosis-Research

Publication Link: https://doi.org/10.3390/axioms12090873

The repository briefly describes the publication I co-authored under Dr. Anass Bouchnita at University of Texas at El Paso where we utilised Deep learning and Machine Learning oriented thrombosis classification research. 

My main contribution towards the research involved running machine learning and deep learning algorithms for the purpose of testing the accuracy of the data we had. My work involved performing binary classification to classify the patients into ones who have clotting and the others that do not have clotting based on the feature ‘clot size’ in our dataset, which was computationally generated.

## Introduction
A blood clot forms in blood veins as a result of the complex process of blood coagulation, which involves biochemical reactions, blood flow, and injured wall. 
Shear stress, flow rate, the presence of platelets, and other clotting factors are among the many variables that interact in this tightly controlled process.
Thrombin generation assays are used to assess the risk of bleeding. However, they are hard to interpret and do not consider in vivo conditions.

## Objective
In this study, we propose a new method for the assessment of the risk of bleeding based on thrombin generation assays using machine learning and mathematical models.
We specifically suggest the fast and accurate determination of the thrombin production levels that initiate coagulation using machine learning models trained with numerical simulations of CFD models.

## Methodology

### Computational Fluid Mechanics Equations
To model thrombus development under flow, a number of methods have been used, including ordinary differentiation equation (ODE), continuum-based modeling, mixture theory, and multiscale modeling, among others.
We used a computational fluid dynamics (CFD) model for coagulation initiation1, and at the same time calculated the following thrombin generation parameters using an ODE Model: 
(i) the lag time, 
(ii) the peak thrombin concentration,
(iii) the time to peak, and (iv) the endogenous thrombin potential (ETP). 
The deep learning model was trained using numerical simulations of the CFD model (ETP).
In the CFD model, we describe blood flow as Newtonian incompressible flow using the Navier-Stokes equation:

<img src="https://github.com/user-attachments/assets/08167575-8d5a-4cd2-a2d0-c0d8cef3b09d" alt="Image Description" width="700" height="100"/>

Blood flow decelerates as it crosses the porous medium, whose porosity depends on the production of fibrin polymer. The production of the latter is described by solving the PDE system:

<img src="https://github.com/user-attachments/assets/a2ba8d63-2a17-4551-affa-79d926f48aef" alt="Image Description" width="400" height="120"/><br>
<img src="https://github.com/user-attachments/assets/4c860290-085a-4f3d-8876-ffbbc9e10bb9" alt="Image Description" width="400" height="300"/>

### Deep Learning and Machine Learning for constucting Surogate Model (_As part of my involvement in the research_)

We develop a surrogate model which consists of an artificial neural network (ANN) that predicts coagulation initiation based on the results of numerical simulations. 
We ran 7675 simulations in total, using 30% to assess the neural network's performance and 70% to train the network.
Simulations are generated by considering random parameters sampled from uniform distributions.
For each simulation, we calculate the thrombin generation parameters: (lag time, ETP, thrombin peak concentration, and time to peak).
Each simulation was given a label of "1" if coagulation started and "0" otherwise.
We consider the thrombin generation parameters as well as blood flow velocity (pressure difference) and the size of the injury site as model inputs.

<img src="https://github.com/user-attachments/assets/2d76078b-3820-4dee-a42f-d8fc7a558dea" alt="Image Description" width="800" height="400"/>

The aforementioned figure also shows the structure of the ANN. To avoid under- and over-fitting, we employed an architecture with three hidden layers and 500 X 250 X 100 nodes. 94% accuracy was attained after 200+ training sessions

# Results (_As part of my involvement in the research_)
<img src="https://github.com/user-attachments/assets/e1b3731e-6ea0-4cbe-993e-d4a55103e160" alt="Image Description" width="400" height="350"/>
<img src="https://github.com/user-attachments/assets/a2406a37-ae53-4524-b099-b1d37aa990b0" alt="Image Description" width="400" height="350"/>
Figure 2 <br>

We trained the neural network for 200 periods and evaluated its performance by
computing metrics such as the multi-cross entropy in log scale and the confusion matrix
(Figure 2). Further, accuracy was evaluated at 94% on the test dataset. The performances
of other NN architectures and classification algorithms. To implement these algorithms, we used the Python libraries Scikit-learn and tensorflow. 

<br>

<img src="https://github.com/user-attachments/assets/e47a4a95-0d0c-4ae0-8fd4-5b79b1f5d8a7" alt="Image Description" width="450" height="450"/>

We compared the accuracy levels attained by different classification algorithms when used on the same dataset for the initiation of coagulation.
We applied different DL algorithms and found that all DNN’s exceeded 90% accuracy. SVM performed better with GridSearch use. Decision tree at depth level – ‘3’ gave 90% accuracy. 
Gradient boosting algorithms did better then most with accuracies surpassing 95%.


